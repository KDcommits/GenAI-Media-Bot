{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import fitz\n",
    "import numpy as np\n",
    "import pinecone as pc\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import  Pinecone ,FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class CustomTextSplitter:\n",
    "\n",
    "    def __init__(self, chunk_size, chunk_overlap):\n",
    "        self.keep_separator= False\n",
    "        self.strip_whitespace=True\n",
    "        self.is_separator_regex=False\n",
    "        self.add_start_index=False\n",
    "        self.length_function = len\n",
    "        self.chunk_size=chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "\n",
    "    def _split_text_with_regex(self,text: str, separator: str):\n",
    "        # Now that we have the separator, split the text\n",
    "        if separator:\n",
    "            if self.keep_separator:\n",
    "                # The parentheses in the pattern keep the delimiters in the result.\n",
    "                _splits = re.split(f\"({separator})\", text)\n",
    "                splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\n",
    "                if len(_splits) % 2 == 0:\n",
    "                    splits += _splits[-1:]\n",
    "                splits = [_splits[0]] + splits\n",
    "            else:\n",
    "                splits = re.split(separator, text)\n",
    "        else:\n",
    "            splits = list(text)\n",
    "        return [s for s in splits if s != \"\"]\n",
    "    \n",
    "    def _join_docs(self, docs, separator:str,):\n",
    "        text = separator.join(docs)\n",
    "        if self.strip_whitespace:\n",
    "            text = text.strip()\n",
    "        if text == \"\":\n",
    "            return None\n",
    "        else:\n",
    "            return text\n",
    "        \n",
    "\n",
    "    def _merge_splits(self, splits, separator: str):\n",
    "        # We now want to combine these smaller pieces into medium size\n",
    "        # chunks to send to the LLM.\n",
    "        separator_len = self.length_function(separator)\n",
    "\n",
    "        docs = []\n",
    "        current_doc= []\n",
    "        total = 0\n",
    "        for d in splits:\n",
    "            _len = self.length_function(d)\n",
    "            if (\n",
    "                total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                > self.chunk_size\n",
    "            ):\n",
    "                if total > self.chunk_size:\n",
    "                    print(\n",
    "                        f\"Created a chunk of size {total}, \"\n",
    "                        f\"which is longer than the specified {self.chunk_size}\"\n",
    "                    )\n",
    "                if len(current_doc) > 0:\n",
    "                    doc = self._join_docs(current_doc, separator)\n",
    "                    if doc is not None:\n",
    "                        docs.append(doc)\n",
    "                    # Keep on popping if:\n",
    "                    # - we have a larger chunk than in the chunk overlap\n",
    "                    # - or if we still have any chunks and the length is long\n",
    "                    while total > self.chunk_overlap or (\n",
    "                        total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                        > self.chunk_size\n",
    "                        and total > 0\n",
    "                    ):\n",
    "                        total -= self.length_function(current_doc[0]) + (\n",
    "                            separator_len if len(current_doc) > 1 else 0\n",
    "                        )\n",
    "                        current_doc = current_doc[1:]\n",
    "            current_doc.append(d)\n",
    "            total += _len + (separator_len if len(current_doc) > 1 else 0)\n",
    "        doc = self._join_docs(current_doc, separator)\n",
    "        if doc is not None:\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "    \n",
    "    def _split_text(self,text: str,):\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        final_chunks = []\n",
    "        # Get appropriate separator to use\n",
    "        separator = self.separators[-1]\n",
    "        new_separators = []\n",
    "        for i, _s in enumerate(self.separators):\n",
    "            _separator = _s if self.is_separator_regex else re.escape(_s)\n",
    "            if _s == \"\":\n",
    "                separator = _s\n",
    "                break\n",
    "            if re.search(_separator, text):\n",
    "                separator = _s\n",
    "                new_separators = self.separators[i + 1 :]\n",
    "                break\n",
    "\n",
    "        _separator = separator if self.is_separator_regex else re.escape(separator)\n",
    "        splits = self._split_text_with_regex(text, _separator)\n",
    "\n",
    "        # Now go merging things, recursively splitting longer texts.\n",
    "        _good_splits = []\n",
    "        _separator = \"\" if self.keep_separator else separator\n",
    "        for s in splits:\n",
    "            if self.length_function(s) < self.chunk_size:\n",
    "                _good_splits.append(s)\n",
    "            else:\n",
    "                if _good_splits:\n",
    "                    merged_text = self._merge_splits(_good_splits, _separator)\n",
    "                    final_chunks.extend(merged_text)\n",
    "                    _good_splits = []\n",
    "                if not new_separators:\n",
    "                    final_chunks.append(s)\n",
    "                else:\n",
    "                    other_info = self._split_text(s, new_separators)\n",
    "                    final_chunks.extend(other_info)\n",
    "        if _good_splits:\n",
    "            merged_text = self._merge_splits(_good_splits, _separator)\n",
    "            final_chunks.extend(merged_text)\n",
    "        return final_chunks\n",
    "    \n",
    "\n",
    "    def create_documents(self, texts, pdf_name):\n",
    "        \"\"\"Create documents from a list of texts.\"\"\"\n",
    "        documents = []\n",
    "        for i, text in enumerate(texts):\n",
    "            index = -1\n",
    "            for chunk in self._split_text(text):\n",
    "                if self.add_start_index:\n",
    "                    index = text.find(chunk, index + 1)\n",
    "                new_doc = Document(page_content=chunk, metadata={})\n",
    "                new_doc.metadata['source'] = pdf_name\n",
    "                documents.append(new_doc)\n",
    "        return documents\n",
    "    \n",
    "class PdfTextExtractor:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path \n",
    "        self.start_page = 1\n",
    "        self.end_page = None\n",
    "\n",
    "    def _preprocess(self,text):\n",
    "        '''\n",
    "        preprocess extrcted text from pdf\n",
    "        1. Replace new line character with whitespace.\n",
    "        2. Replace redundant whitespace with a single whitespace\n",
    "        '''\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        text = re.sub(r'\\\\u[e-f][0-9a-z]{3}',' ', text)\n",
    "        return text\n",
    "    \n",
    "    def _pdf_to_text(self, pdf_filename):\n",
    "        '''\n",
    "            convert pdf to a list of words.\n",
    "        '''\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        total_pages= doc.page_count\n",
    "\n",
    "        if self.end_page is None:\n",
    "            self.end_page = total_pages\n",
    "        text_list=[]\n",
    "\n",
    "        for i in tqdm(range(self.start_page-1, self.end_page)):\n",
    "            text= doc.load_page(i).get_text('text')\n",
    "            text= self._preprocess(text)\n",
    "            text_list.append(f'[{pdf_filename}, page:{i+1}] '+text+ f' [{pdf_filename}, page:{i+1}]')\n",
    "        doc.close()\n",
    "        return text_list\n",
    "    \n",
    "\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, pdf_data_path, vector_db_path):\n",
    "\n",
    "        self.pdf_data_path = pdf_data_path\n",
    "        self.vector_db_path = vector_db_path\n",
    "        self.pinecone_api_key = os.getenv('PINECONE_KEY')\n",
    "        self.pinecone_env = os.getenv('PINECONE_ENV')\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "        self.openai_embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002',\n",
    "                                                        openai_api_key=os.getenv('OPENAI_KEY'))\n",
    "        \n",
    "    def createPDFVectorDBwithFAISS(self, chunk_size, chunk_overlap):\n",
    "        document_list=[]\n",
    "        for pdf_filename in os.listdir(self.pdf_data_path):\n",
    "            pdf_file_path = os.path.join(self.pdf_data_path,pdf_filename)\n",
    "            extracted_text_list = PdfTextExtractor(pdf_file_path)._pdf_to_text(pdf_filename)\n",
    "            merged_text_list = ['.'.join(extracted_text_list)]\n",
    "            splitter = CustomTextSplitter(chunk_size, chunk_overlap)\n",
    "            docs  = splitter.create_documents(merged_text_list,pdf_filename)\n",
    "            document_list.extend(docs)\n",
    "    \n",
    "        db = FAISS.from_documents(document_list, self.embedding_model)\n",
    "        db.save_local(self.vector_db_path)\n",
    "\n",
    "    def create_top_k_chunk_from_FAISS(self, question,top_k):\n",
    "        test_idex = FAISS.load_local(self.vector_db_path,self.embedding_model)\n",
    "        top_k_chunks  = test_idex.similarity_search(question,k=top_k)\n",
    "        return top_k_chunks\n",
    "    \n",
    "    def fetch_FAISS_VectorDB(self):\n",
    "        test_index = FAISS.load_local(self.vector_db_path,self.embedding_model)\n",
    "        return test_index\n",
    "    \n",
    "\n",
    "    def createPDFVectorDBwithPinecone(self,chunk_size, chunk_overlap):\n",
    "        document_list=[]\n",
    "        for pdf_filename in os.listdir(self.pdf_data_path):\n",
    "            pdf_file_path = os.path.join(self.pdf_data_path,pdf_filename)\n",
    "            extracted_text_list = PdfTextExtractor(pdf_file_path)._pdf_to_text(pdf_filename)\n",
    "            merged_text_list = ['.'.join(extracted_text_list)]\n",
    "            splitter = CustomTextSplitter(chunk_size, chunk_overlap)\n",
    "            docs  = splitter.create_documents(merged_text_list,pdf_filename)\n",
    "            document_list.extend(docs)\n",
    "\n",
    "        embeddings = []\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        for i in range(len(document_list)):\n",
    "            if i%5==0:\n",
    "                time.sleep(5)\n",
    "            page_content = document_list[i].page_content\n",
    "            source_pdf = document_list[i].metadata['source'].split('\\\\')[-1]\n",
    "            embedded_page_content = self.openai_embedding_model.embed_query(page_content)\n",
    "            metadata = {\n",
    "                'source' : source_pdf,\n",
    "                'page_content' : page_content\n",
    "            }\n",
    "            ids.append(str(i))\n",
    "            embeddings.append(embedded_page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "        pc.init(api_key=self.pinecone_api_key, environment=self.pinecone_env)\n",
    "        index = pc.Index('pdf-index')\n",
    "        index.upsert(vectors = zip(ids, embeddings, metadatas))\n",
    "\n",
    "    def create_top_k_chunk_from_Pinecone(self, question,top_k):\n",
    "        pc.init(api_key=self.pinecone_api_key, environment=self.pinecone_env)\n",
    "        index = pc.Index('pdf-index')\n",
    "        vectorstore = Pinecone( index, self.openai_embedding_model.embed_query, text_key='page_content')\n",
    "        top_k_chunks = vectorstore.similarity_search(question, k=top_k)\n",
    "        return top_k_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:00<00:00, 434.46it/s]\n",
      "100%|██████████| 281/281 [00:01<00:00, 243.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 351.13it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf_data_path = \".\\\\media\"\n",
    "pdf_vector_embedding_path = \".\\\\VectorDB\"\n",
    "data_obj = Data(pdf_data_path,pdf_vector_embedding_path)\n",
    "data_obj.createPDFVectorDBwithFAISS(chunk_size=2000, chunk_overlap=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='key to a nation’s progress. Our government is committed to providing a transparent and accountable administration which works for the betterment and welfare of the common citizen,” said Hon’ble Prime Minister. [budget_2023.pdf, page:17].[budget_2023.pdf, page:18] 14 Mission Karmayogi 58. Under Mission Karmayogi, Centre, States and Union Territories are making and implementing capacity-building plans for civil servants. The government has also launched an integrated online training platform, iGOT Karmayogi, to provide continuous learning opportunities for lakhs of government employees to upgrade their skills and facilitate people-centric approach. 59. For enhancing ease of doing business, more than 39,000 compliances have been reduced and more than 3,400 legal provisions have been decriminalized. For furthering the trust- based governance, we have introduced the Jan Vishwas Bill to amend 42 Central Acts. This Budget proposes a series of measures to unleash the potential of our economy. Centres of Excellence for Artificial Intelligence 60. For realizing the vision of “Make AI in India and Make AI work for India”, three centres of excellence for Artificial Intelligence will be set-up in top educational institutions. Leading industry players will partner in conducting interdisciplinary research, develop cutting-edge applications and scalable problem solutions in the areas of agriculture, health, and sustainable cities. This will galvanize an effective AI ecosystem and nurture quality human resources in the field. National Data Governance Policy 61. To unleash innovation and research by start-ups and academia, a National Data Governance Policy will be brought out. This will enable access to anonymized data. Simplification of Know Your Customer (KYC) process 62. The KYC process will be simplified adopting a ‘risk-based’ instead of ‘one size fits all’ approach. The financial sector regulators will also be [budget_2023.pdf, page:18].[budget_2023.pdf, page:19] 15 encouraged', metadata={'source': 'budget_2023.pdf'}),\n",
       " Document(page_content='Inscriptions’ will be set up in a digital epigraphy museum, with digitization of one lakh ancient inscriptions in the first stage. Support for poor prisoners 42. For poor persons who are in prisons and unable to afford the penalty or the bail amount, required financial support will be provided. Priority 3: Infrastructure & Investment 43. Investments in Infrastructure and productive capacity have a large multiplier impact on growth and employment. After the subdued period of the pandemic, private investments are growing again. The Budget takes the lead once again to ramp up the virtuous cycle of investment and job creation. Capital Investment as driver of growth and jobs 44. Capital investment outlay is being increased steeply for the third year in a row by 33 per cent to ` 10 lakh crore, which would be 3.3 per cent of GDP. This will be almost three times the outlay in 2019-20. 45. This substantial increase in recent years is central to the government’s efforts to enhance growth potential and job creation, crowd- in private investments, and provide a cushion against global headwinds. Effective Capital Expenditure 46. The direct capital investment by the Centre is complemented by the provision made for creation of capital assets through Grants-in-Aid to States. The ‘Effective Capital Expenditure’ of the Centre is budgeted at ` 13.7 lakh crore, which will be 4.5 per cent of GDP. [budget_2023.pdf, page:15].[budget_2023.pdf, page:16] 12 Support to State Governments for Capital Investment 47. I have decided to continue the 50-year interest free loan to state governments for one more year to spur investment in infrastructure and to incentivize them for complementary policy actions, with a significantly enhanced outlay of ` 1.3 lakh crore. Enhancing opportunities for private investment in Infrastructure 48. The newly established Infrastructure Finance Secretariat will assist all stakeholders for more private investment in infrastructure, including railways, roads, urban', metadata={'source': 'budget_2023.pdf'}),\n",
       " Document(page_content='payments, and social security. This will greatly benefit the Scheduled Castes, Scheduled Tribes, OBCs, women and people belonging to the weaker sections. 3) Tourism: The country offers immense attraction for domestic as well as foreign tourists. There is a large potential to be tapped in tourism. The sector holds huge opportunities for jobs and entrepreneurship for youth in particular. Promotion of tourism will be taken up on mission mode, with active participation of states, convergence of government programmes and public-private partnerships. [budget_2023.pdf, page:8].[budget_2023.pdf, page:9] 5 4) Green Growth: We are implementing many programmes for green fuel, green energy, green farming, green mobility, green buildings, and green equipment, and policies for efficient use of energy across various economic sectors. These green growth efforts help in reducing carbon intensity of the economy and provides for large- scale green job opportunities. Priorities of this Budget 14. The Budget adopts the following seven priorities. They complement each other and act as the ‘Saptarishi’ guiding us through the Amrit Kaal. 1) Inclusive Development 2) Reaching the Last Mile 3) Infrastructure and Investment 4) Unleashing the Potential 5) Green Growth 6) Youth Power 7) Financial Sector Priority 1: Inclusive Development 15. The Government’s philosophy of Sabka Saath Sabka Vikas has facilitated inclusive development covering in specific, farmers, women, youth, OBCs, Scheduled Castes, Scheduled Tribes, divyangjan and economically weaker sections, and overall priority for the underprivileged (vanchiton ko variyata). There has also been a sustained focus on Jammu & Kashmir, Ladakh and the North-East. This Budget builds on those efforts. Agriculture and Cooperation Digital Public Infrastructure for Agriculture 16. Digital public infrastructure for agriculture will be built as an open source, open standard and inter operable public good. This will enable [budget_2023.pdf,', metadata={'source': 'budget_2023.pdf'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_question = \"Artificial Intelligence in budget?\"\n",
    "result = data_obj.create_top_k_chunk_from_FAISS(test_question, top_k =3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='key to a nation’s progress. Our government is committed to providing a transparent and accountable administration which works for the betterment and welfare of the common citizen,” said Hon’ble Prime Minister. [budget_2023.pdf, page:17].[budget_2023.pdf, page:18] 14 Mission Karmayogi 58. Under Mission Karmayogi, Centre, States and Union Territories are making and implementing capacity-building plans for civil servants. The government has also launched an integrated online training platform, iGOT Karmayogi, to provide continuous learning opportunities for lakhs of government employees to upgrade their skills and facilitate people-centric approach. 59. For enhancing ease of doing business, more than 39,000 compliances have been reduced and more than 3,400 legal provisions have been decriminalized. For furthering the trust- based governance, we have introduced the Jan Vishwas Bill to amend 42 Central Acts. This Budget proposes a series of measures to unleash the potential of our economy. Centres of Excellence for Artificial Intelligence 60. For realizing the vision of “Make AI in India and Make AI work for India”, three centres of excellence for Artificial Intelligence will be set-up in top educational institutions. Leading industry players will partner in conducting interdisciplinary research, develop cutting-edge applications and scalable problem solutions in the areas of agriculture, health, and sustainable cities. This will galvanize an effective AI ecosystem and nurture quality human resources in the field. National Data Governance Policy 61. To unleash innovation and research by start-ups and academia, a National Data Governance Policy will be brought out. This will enable access to anonymized data. Simplification of Know Your Customer (KYC) process 62. The KYC process will be simplified adopting a ‘risk-based’ instead of ‘one size fits all’ approach. The financial sector regulators will also be [budget_2023.pdf, page:18].[budget_2023.pdf, page:19] 15 encouraged', metadata={'source': 'budget_2023.pdf'}),\n",
       " Document(page_content='Inscriptions’ will be set up in a digital epigraphy museum, with digitization of one lakh ancient inscriptions in the first stage. Support for poor prisoners 42. For poor persons who are in prisons and unable to afford the penalty or the bail amount, required financial support will be provided. Priority 3: Infrastructure & Investment 43. Investments in Infrastructure and productive capacity have a large multiplier impact on growth and employment. After the subdued period of the pandemic, private investments are growing again. The Budget takes the lead once again to ramp up the virtuous cycle of investment and job creation. Capital Investment as driver of growth and jobs 44. Capital investment outlay is being increased steeply for the third year in a row by 33 per cent to ` 10 lakh crore, which would be 3.3 per cent of GDP. This will be almost three times the outlay in 2019-20. 45. This substantial increase in recent years is central to the government’s efforts to enhance growth potential and job creation, crowd- in private investments, and provide a cushion against global headwinds. Effective Capital Expenditure 46. The direct capital investment by the Centre is complemented by the provision made for creation of capital assets through Grants-in-Aid to States. The ‘Effective Capital Expenditure’ of the Centre is budgeted at ` 13.7 lakh crore, which will be 4.5 per cent of GDP. [budget_2023.pdf, page:15].[budget_2023.pdf, page:16] 12 Support to State Governments for Capital Investment 47. I have decided to continue the 50-year interest free loan to state governments for one more year to spur investment in infrastructure and to incentivize them for complementary policy actions, with a significantly enhanced outlay of ` 1.3 lakh crore. Enhancing opportunities for private investment in Infrastructure 48. The newly established Infrastructure Finance Secretariat will assist all stakeholders for more private investment in infrastructure, including railways, roads, urban', metadata={'source': 'budget_2023.pdf'}),\n",
       " Document(page_content='payments, and social security. This will greatly benefit the Scheduled Castes, Scheduled Tribes, OBCs, women and people belonging to the weaker sections. 3) Tourism: The country offers immense attraction for domestic as well as foreign tourists. There is a large potential to be tapped in tourism. The sector holds huge opportunities for jobs and entrepreneurship for youth in particular. Promotion of tourism will be taken up on mission mode, with active participation of states, convergence of government programmes and public-private partnerships. [budget_2023.pdf, page:8].[budget_2023.pdf, page:9] 5 4) Green Growth: We are implementing many programmes for green fuel, green energy, green farming, green mobility, green buildings, and green equipment, and policies for efficient use of energy across various economic sectors. These green growth efforts help in reducing carbon intensity of the economy and provides for large- scale green job opportunities. Priorities of this Budget 14. The Budget adopts the following seven priorities. They complement each other and act as the ‘Saptarishi’ guiding us through the Amrit Kaal. 1) Inclusive Development 2) Reaching the Last Mile 3) Infrastructure and Investment 4) Unleashing the Potential 5) Green Growth 6) Youth Power 7) Financial Sector Priority 1: Inclusive Development 15. The Government’s philosophy of Sabka Saath Sabka Vikas has facilitated inclusive development covering in specific, farmers, women, youth, OBCs, Scheduled Castes, Scheduled Tribes, divyangjan and economically weaker sections, and overall priority for the underprivileged (vanchiton ko variyata). There has also been a sustained focus on Jammu & Kashmir, Ladakh and the North-East. This Budget builds on those efforts. Agriculture and Cooperation Digital Public Infrastructure for Agriculture 16. Digital public infrastructure for agriculture will be built as an open source, open standard and inter operable public good. This will enable [budget_2023.pdf,', metadata={'source': 'budget_2023.pdf'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data_path = \".\\\\media\"\n",
    "pdf_vector_embedding_path = \".\\\\VectorDB\"\n",
    "data_obj = Data(pdf_data_path,pdf_vector_embedding_path)\n",
    "test_question = \"Artificial Intelligence in budget?\"\n",
    "result = data_obj.create_top_k_chunk_from_FAISS(test_question, top_k=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_KEY\")\n",
    "        self.model = 'text-davinci-003'\n",
    "        self.llm = ChatOpenAI( openai_api_key = os.getenv(\"OPENAI_KEY\"))\n",
    "\n",
    "    def createQuestionPrompt(self, top_k_chunks):\n",
    "        prompt= \"\"\n",
    "        prompt += 'Context:\\n\\n'\n",
    "        for i in range(len(top_k_chunks)):\n",
    "            page_content = top_k_chunks[i].page_content.replace('\\n', ' ')\n",
    "            page_content = re.sub('\\s+', ' ', page_content)\n",
    "            prompt += page_content +'\\n\\n'\n",
    "        prompt += '''\\nInstructions: Compose a comprehensive reply to the query asked by the user from the context provided.\n",
    "        Cite each reference using [pdfname.pdf , page : number] notation (every result has this number at the beginning and end).\n",
    "        Citation should be done at the end of each sentence. If the search results mention multiple subjects\n",
    "        with the same name, create separate answers for each. Only include information found in the results and\n",
    "        don't add any additional information. Make sure the answer is correct and don't output false content.\n",
    "        If the text does not relate to the query, simply state 'Found Nothing'. Don't write 'Answer:'\n",
    "        Directly start the answer.\\n'''.replace('\\\\n',' ')\n",
    "    \n",
    "        return prompt\n",
    "    \n",
    "    def createQuestionPromptTemplate(self, prompt):\n",
    "        prompt_template_llmchain = ChatPromptTemplate.from_messages([\n",
    "                    SystemMessage(content=f\"You are a QnA Chatbot whose job is to greet the user politely and asnwer to the question they ask.\"+prompt), \n",
    "                    MessagesPlaceholder(variable_name=\"chat_history\"),         # Where the memory will be stored.\n",
    "                    HumanMessagePromptTemplate.from_template(\"{human_input}\"), # Where the human input will injected\n",
    "                 ])\n",
    "        \n",
    "        return prompt_template_llmchain\n",
    "    \n",
    "    def generateAnswer(self, prompt):\n",
    "        openai.api_key = self.openai_api_key\n",
    "        completions = openai.Completion.create(\n",
    "            engine=self.model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0,\n",
    "        )\n",
    "        answer = completions.choices[0]['text']\n",
    "        return answer\n",
    "    \n",
    "    def generateAnswerwithMemory(self,question, prompt_template, chat_history):\n",
    "        memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=True, k=3)\n",
    "        llm = ChatOpenAI(openai_api_key = os.getenv(\"OPENAI_KEY\"),model='gpt-3.5-turbo')\n",
    "        llm_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt_template,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        llm_chain_response = llm_chain.predict(human_input = question)\n",
    "        interaction = 'human:'+question+'\\nchatbot:'+llm_chain_response\n",
    "        chat_history.append(interaction)\n",
    "        return llm_chain_response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a QnA Chatbot whose job is to greet the user politely and asnwer to the question they ask.Context:\n",
      "\n",
      "key to a nation’s progress. Our government is committed to providing a transparent and accountable administration which works for the betterment and welfare of the common citizen,” said Hon’ble Prime Minister. [budget_2023.pdf, page:17].[budget_2023.pdf, page:18] 14 Mission Karmayogi 58. Under Mission Karmayogi, Centre, States and Union Territories are making and implementing capacity-building plans for civil servants. The government has also launched an integrated online training platform, iGOT Karmayogi, to provide continuous learning opportunities for lakhs of government employees to upgrade their skills and facilitate people-centric approach. 59. For enhancing ease of doing business, more than 39,000 compliances have been reduced and more than 3,400 legal provisions have been decriminalized. For furthering the trust- based governance, we have introduced the Jan Vishwas Bill to amend 42 Central Acts. This Budget proposes a series of measures to unleash the potential of our economy. Centres of Excellence for Artificial Intelligence 60. For realizing the vision of “Make AI in India and Make AI work for India”, three centres of excellence for Artificial Intelligence will be set-up in top educational institutions. Leading industry players will partner in conducting interdisciplinary research, develop cutting-edge applications and scalable problem solutions in the areas of agriculture, health, and sustainable cities. This will galvanize an effective AI ecosystem and nurture quality human resources in the field. National Data Governance Policy 61. To unleash innovation and research by start-ups and academia, a National Data Governance Policy will be brought out. This will enable access to anonymized data. Simplification of Know Your Customer (KYC) process 62. The KYC process will be simplified adopting a ‘risk-based’ instead of ‘one size fits all’ approach. The financial sector regulators will also be [budget_2023.pdf, page:18].[budget_2023.pdf, page:19] 15 encouraged\n",
      "\n",
      "Inscriptions’ will be set up in a digital epigraphy museum, with digitization of one lakh ancient inscriptions in the first stage. Support for poor prisoners 42. For poor persons who are in prisons and unable to afford the penalty or the bail amount, required financial support will be provided. Priority 3: Infrastructure & Investment 43. Investments in Infrastructure and productive capacity have a large multiplier impact on growth and employment. After the subdued period of the pandemic, private investments are growing again. The Budget takes the lead once again to ramp up the virtuous cycle of investment and job creation. Capital Investment as driver of growth and jobs 44. Capital investment outlay is being increased steeply for the third year in a row by 33 per cent to ` 10 lakh crore, which would be 3.3 per cent of GDP. This will be almost three times the outlay in 2019-20. 45. This substantial increase in recent years is central to the government’s efforts to enhance growth potential and job creation, crowd- in private investments, and provide a cushion against global headwinds. Effective Capital Expenditure 46. The direct capital investment by the Centre is complemented by the provision made for creation of capital assets through Grants-in-Aid to States. The ‘Effective Capital Expenditure’ of the Centre is budgeted at ` 13.7 lakh crore, which will be 4.5 per cent of GDP. [budget_2023.pdf, page:15].[budget_2023.pdf, page:16] 12 Support to State Governments for Capital Investment 47. I have decided to continue the 50-year interest free loan to state governments for one more year to spur investment in infrastructure and to incentivize them for complementary policy actions, with a significantly enhanced outlay of ` 1.3 lakh crore. Enhancing opportunities for private investment in Infrastructure 48. The newly established Infrastructure Finance Secretariat will assist all stakeholders for more private investment in infrastructure, including railways, roads, urban\n",
      "\n",
      "payments, and social security. This will greatly benefit the Scheduled Castes, Scheduled Tribes, OBCs, women and people belonging to the weaker sections. 3) Tourism: The country offers immense attraction for domestic as well as foreign tourists. There is a large potential to be tapped in tourism. The sector holds huge opportunities for jobs and entrepreneurship for youth in particular. Promotion of tourism will be taken up on mission mode, with active participation of states, convergence of government programmes and public-private partnerships. [budget_2023.pdf, page:8].[budget_2023.pdf, page:9] 5 4) Green Growth: We are implementing many programmes for green fuel, green energy, green farming, green mobility, green buildings, and green equipment, and policies for efficient use of energy across various economic sectors. These green growth efforts help in reducing carbon intensity of the economy and provides for large- scale green job opportunities. Priorities of this Budget 14. The Budget adopts the following seven priorities. They complement each other and act as the ‘Saptarishi’ guiding us through the Amrit Kaal. 1) Inclusive Development 2) Reaching the Last Mile 3) Infrastructure and Investment 4) Unleashing the Potential 5) Green Growth 6) Youth Power 7) Financial Sector Priority 1: Inclusive Development 15. The Government’s philosophy of Sabka Saath Sabka Vikas has facilitated inclusive development covering in specific, farmers, women, youth, OBCs, Scheduled Castes, Scheduled Tribes, divyangjan and economically weaker sections, and overall priority for the underprivileged (vanchiton ko variyata). There has also been a sustained focus on Jammu & Kashmir, Ladakh and the North-East. This Budget builds on those efforts. Agriculture and Cooperation Digital Public Infrastructure for Agriculture 16. Digital public infrastructure for agriculture will be built as an open source, open standard and inter operable public good. This will enable [budget_2023.pdf,\n",
      "\n",
      "\n",
      "Instructions: Compose a comprehensive reply to the query asked by the user from the context provided.\n",
      "        Cite each reference using [pdfname.pdf , page : number] notation (every result has this number at the beginning and end).\n",
      "        Citation should be done at the end of each sentence. If the search results mention multiple subjects\n",
      "        with the same name, create separate answers for each. Only include information found in the results and\n",
      "        don't add any additional information. Make sure the answer is correct and don't output false content.\n",
      "        If the text does not relate to the query, simply state 'Found Nothing'. Don't write 'Answer:'\n",
      "        Directly start the answer.\n",
      "\n",
      "Human: Artificial Intelligence in budget?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Three centers of excellence for Artificial Intelligence will be set up in top educational institutions as part of the budget. These centers will partner with leading industry players to conduct interdisciplinary research, develop cutting-edge applications, and provide scalable problem solutions in the areas of agriculture, health, and sustainable cities. The aim is to build an effective AI ecosystem and nurture quality human resources in this field [budget_2023.pdf, page:18].'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = Model()\n",
    "prompt = model_obj.createQuestionPrompt(result)\n",
    "prompt_template = model_obj.createQuestionPromptTemplate(prompt)\n",
    "response = model_obj.generateAnswerwithMemory(test_question, prompt_template,chat_history=[])\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector as connection \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_KEY')\n",
    "openai.api_key = os.getenv('OPENAI_KEY')\n",
    "\n",
    "class SQLQuerywithLangchain:\n",
    "    def __init__(self):\n",
    "        self.DB_USERNAME = os.getenv('DB_USERNAME')\n",
    "        self.DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "        self.DB_HOST = os.getenv('DB_HOST')\n",
    "        self.DB_PORT = os.getenv('DB_PORT')\n",
    "        self.DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "    def createDBConnectionString(self):\n",
    "        db_user = self.DB_USERNAME\n",
    "        db_Password  = self.DB_PASSWORD\n",
    "        db_host = self.DB_HOST + self.DB_PORT\n",
    "        db_name = self.DB_NAME\n",
    "        connectionString = f\"mysql+pymysql://{db_user}:{db_Password}@{db_host}/{db_name}\"\n",
    "        return connectionString\n",
    "    \n",
    "    def getSQLSchema(self):\n",
    "            ''''\n",
    "                Extracting the schema info from the MySQL database and passing the schema \n",
    "                information to the prompt.\n",
    "            '''\n",
    "            sql_query = f\"\"\"  \n",
    "            SELECT C.TABLE_NAME, C.COLUMN_NAME, C.DATA_TYPE, T.TABLE_TYPE, T.TABLE_SCHEMA  \n",
    "            FROM INFORMATION_SCHEMA.COLUMNS C  \n",
    "            JOIN INFORMATION_SCHEMA.TABLES T ON C.TABLE_NAME = T.TABLE_NAME AND C.TABLE_SCHEMA = T.TABLE_SCHEMA  \n",
    "            WHERE T.TABLE_SCHEMA = '{self.DB_NAME}' \n",
    "            \"\"\" \n",
    "            mysql_connection_string = self.createDBConnectionString()\n",
    "            result = pd.read_sql_query(sql_query, mysql_connection_string)\n",
    "            df = result.infer_objects()\n",
    "            output=[]\n",
    "            current_table = ''  \n",
    "            columns = []  \n",
    "            for index, row in df.iterrows():\n",
    "                table_name = f\"{row['TABLE_SCHEMA']}.{row['TABLE_NAME']}\"  \n",
    "                column_name = row['COLUMN_NAME']  \n",
    "                data_type = row['DATA_TYPE']  \n",
    "                if \" \" in table_name:\n",
    "                    table_name= f\"[{table_name}]\" \n",
    "                column_name = row['COLUMN_NAME']  \n",
    "                if \" \" in column_name:\n",
    "                    column_name= f\"[{column_name}]\" \n",
    "                    # If the table name has changed, output the previous table's information  \n",
    "                if current_table != table_name and current_table != '':  \n",
    "                    output.append(f\"table: {current_table}, columns: {', '.join(columns)}\")  \n",
    "                    columns = []  \n",
    "                \n",
    "                # Add the current column information to the list of columns for the current table  \n",
    "                columns.append(f\"{column_name} {data_type}\")  \n",
    "                \n",
    "                # Update the current table name  \n",
    "                current_table = table_name  \n",
    "\n",
    "            # Output the last table's information  \n",
    "            output.append(f\"table: {current_table}, columns: {', '.join(columns)}\")\n",
    "            output = \"\\n \".join(output)\n",
    "\n",
    "            return output   \n",
    "\n",
    "    def createAgentExecutor(self, openAI_model_name=\"gpt-3.5-turbo\"):\n",
    "        '''\n",
    "            Instantiating Langchain agent to query SQL Database.\n",
    "            Using SQLDatabaseToolkit from Langchain.\n",
    "        '''\n",
    "        mysql_connection_string = self.createDBConnectionString()\n",
    "        llm = ChatOpenAI(model_name= openAI_model_name )\n",
    "        db = SQLDatabase.from_uri(mysql_connection_string)\n",
    "        toolkit = SQLDatabaseToolkit(db=db, llm =llm)\n",
    "        agent_executor = create_sql_agent(\n",
    "                                        llm=llm,\n",
    "                                        toolkit=toolkit,\n",
    "                                        verbose=True,\n",
    "                                        return_intermediate_steps=False,\n",
    "                                        handle_parsing_errors=True)\n",
    "        return agent_executor\n",
    "\n",
    "    def fetchQueryResult(self,question):\n",
    "        '''\n",
    "            Using langchain's SQL tool to fetch answer to the user's query.\n",
    "        '''\n",
    "        db_agent = self.createAgentExecutor()\n",
    "        schema_info = self.getSQLSchema()\n",
    "        prompt = f'''You are a professional SQL Data Analyst whose job is to fetch results from the SQL database.\\\n",
    "            The SQL Table schema is as follows {schema_info}.\\\n",
    "            The question will be asked in # delimiter. If you are not able to find the answer write \"Found Nothing\" in response.\\\n",
    "            Do not write anything out of context or on your own.\\\n",
    "            If the SQL query returns multiple rows then summarize them and provide response using bullet points.For duplicate response after the SQL query consider any one of the result to parse into LLM.\\\n",
    "            Question : # {question} #'''\n",
    "        db_agent.return_intermediate_steps=True\n",
    "        agent_response = db_agent(prompt)\n",
    "        output = agent_response['output']\n",
    "        # query = agent_response['intermediate_steps'][-1][0].log.split('\\n')[-1].split('Action Input:')[-1].strip().strip('\"')\n",
    "        return output \n",
    "    \n",
    "\n",
    "class SQLQuerywithFunctionCalling(SQLQuerywithLangchain):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def getMYSQLConnectionObject(self):\n",
    "        db_user = self.DB_USERNAME\n",
    "        db_password  = self.DB_PASSWORD\n",
    "        db_host = self.DB_HOST\n",
    "        db_name = self.DB_NAME\n",
    "        conn = connection.connect(host=db_host,user=db_user,password=db_password,\n",
    "                                        database=db_name, use_pure=True) \n",
    "        if conn.is_connected():\n",
    "            return conn\n",
    "        else:\n",
    "            return \"Database connection can't be established\"\n",
    "    \n",
    "    def defineFunction(self):\n",
    "        database_schema_string = self.getSQLSchema()\n",
    "        function = [\n",
    "            {\n",
    "                \"name\": \"ask_database\",\n",
    "                \"description\": \"Use this function to answer user questions about product. Output should be a fully formed SQL query.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": f\"\"\"\n",
    "                                    SQL query extracting info to answer the user's question.\n",
    "                                    SQL should be written using this database schema:\n",
    "                                    {database_schema_string}\n",
    "                                    The query should be returned in plain text, not in JSON.\n",
    "                                    Do not use new lines chatacthers inside the query.\n",
    "                                    \"\"\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return function\n",
    "    \n",
    "    def ask_database(self,query):\n",
    "        \"\"\"Function to query MySQL database with a provided SQL query.\"\"\"\n",
    "        try:\n",
    "            conn = self.getMYSQLConnectionObject()\n",
    "            cursor=conn.cursor()   \n",
    "            cursor.execute(query)  \n",
    "            results = str(cursor.fetchall())\n",
    "            conn.close()\n",
    "        except Exception as e:\n",
    "            results = f\"query failed with error: {e}\"\n",
    "        return results\n",
    "    \n",
    "    def execute_function_call(self,message):\n",
    "        if message[\"function_call\"][\"name\"] == \"ask_database\":\n",
    "            query = eval(message[\"function_call\"][\"arguments\"])[\"query\"]\n",
    "            results = self.ask_database(query)\n",
    "        else:\n",
    "            results = f\"Error: function {message['function_call']['name']} does not exist\"\n",
    "        return results\n",
    "    \n",
    "    def openai_functions_chain(self,query):\n",
    "        messages = []\n",
    "        messages.append({\"role\": \"system\", \"content\": F\"Answer user questions by generating SQL queries against the {self.DB_NAME} Database.\"})\n",
    "        messages.append({\"role\": \"user\", \"content\": query})\n",
    "        while True:\n",
    "            assistant_message = openai.ChatCompletion.create(\n",
    "                temperature=0,\n",
    "                model=\"gpt-3.5-turbo-0613\",\n",
    "                messages=messages,\n",
    "                functions=self.defineFunction(),\n",
    "                function_call=\"auto\",\n",
    "            )[\"choices\"][0][\"message\"]\n",
    "            messages.append(assistant_message)\n",
    "\n",
    "            if assistant_message.get(\"function_call\"):\n",
    "                print(\"Executing function: \", assistant_message[\"function_call\"])\n",
    "                results = self.execute_function_call(assistant_message)\n",
    "                messages.append({\"role\": \"function\", \"name\": assistant_message[\"function_call\"][\"name\"], \"content\": results})\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return assistant_message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing function:  {\n",
      "  \"name\": \"ask_database\",\n",
      "  \"arguments\": \"{\\n  \\\"query\\\": \\\"SELECT COUNT(*) FROM mavenfuzzyfactory.website_sessions WHERE device_type = 'mobile'\\\"\\n}\"\n",
      "}\n",
      "The count of mobile device type is 145,844.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the count of mobile device type?\"\n",
    "obj = SQLQuerywithFunctionCalling()\n",
    "res = obj.openai_functions_chain(question)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test- Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_recording :  C:\\Users\\krish\\OneDrive\\Desktop\\Study\\Gen AI\\Gen AI - MediaBot\\Data\\Input\\Input Audio\\recorded_audio.wav\n",
      "latest_recording_filename :  recorded_audio.wav\n",
      "Product table is unique product.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Product table is unique product.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whisper\n",
    "import os, glob\n",
    "\n",
    "def speech2Text(recorded_audio_path, transcripted_audio_path):\n",
    "    try:\n",
    "        recordings_dir = os.path.join(recorded_audio_path,'*')\n",
    "        model = whisper.load_model(\"base\")\n",
    "\n",
    "        # get most recent wav recording in the recordings directory\n",
    "        files = sorted(glob.iglob(recordings_dir), key=os.path.getctime, reverse=True)\n",
    "        latest_recording = files[0]\n",
    "        print(\"latest_recording : \", latest_recording)\n",
    "        latest_recording_filename = latest_recording.split('\\\\')[-1]\n",
    "        print(\"latest_recording_filename : \", latest_recording_filename)\n",
    "\n",
    "        if os.path.exists(latest_recording):\n",
    "            audio = whisper.load_audio(latest_recording)\n",
    "            audio = whisper.pad_or_trim(audio)\n",
    "            mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "            options = whisper.DecodingOptions(language= 'en', fp16=False)\n",
    "\n",
    "            result = whisper.decode(model, mel, options)\n",
    "\n",
    "            if result.no_speech_prob < 0.5:\n",
    "                print(result.text)\n",
    "                textfiles = os.listdir(transcripted_audio_path)\n",
    "                if len(textfiles)!=0:\n",
    "                    os.remove(os.path.join(transcripted_audio_path,textfiles[0]))\n",
    "\n",
    "                transcription_filename = os.path.join(transcripted_audio_path,'input_audio_transcription.txt')#'Stored Data/Input/Transcription/input_audio_transcription.txt'\n",
    "                with open(transcription_filename, 'a') as f:\n",
    "                    f.write(result.text)\n",
    "\n",
    "                return result.text\n",
    "            \n",
    "            else:\n",
    "                alternate_text = \"The input audio contains no word\"\n",
    "                textfiles = os.listdir(transcripted_audio_path)\n",
    "                if len(textfiles)!=0:\n",
    "                    os.remove(os.path.join(transcripted_audio_path,textfiles[0]))\n",
    "\n",
    "                transcription_filename = os.path.join(transcripted_audio_path,'input_audio_transcription.txt')#'Stored Data/Input/Transcription/input_audio_transcription.txt'\n",
    "                with open(transcription_filename, 'a') as f:\n",
    "                    f.write(alternate_text)\n",
    "\n",
    "                return alternate_text\n",
    "\n",
    "    except:\n",
    "        print(\"Error! Check 'speech2Text' function\")\n",
    "\n",
    "recorded_audio_path = \"C:\\\\Users\\\\krish\\\\OneDrive\\\\Desktop\\\\Study\\\\Gen AI\\\\Gen AI - MediaBot\\\\Data\\\\Input\\\\Input Audio\\\\\"\n",
    "transcripted_audio_path = 'C:\\\\Users\\\\krish\\\\OneDrive\\\\Desktop\\\\Study\\\\Gen AI\\\\Gen AI - MediaBot\\\\Data\\\\Output\\\\Audio Transcript\\\\'\n",
    "speech2Text(recorded_audio_path,transcripted_audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_audio_path = \"C:\\\\Users\\\\krish\\\\OneDrive\\\\Desktop\\\\Study\\\\Gen AI\\\\Gen AI - MediaBot\\\\Data\\\\Input\\\\Input Audio\\\\recorded_audio.wav\"\n",
    "model = whisper.load_model(\"base\")\n",
    "audio = whisper.load_audio(recorded_audio_path)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, probs = model.detect_language(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bn', 0.5023839473724365)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = sorted(probs.items(), key=lambda x:x[1])[-1]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = whisper.DecodingOptions(language= 'bn', fp16=False)\n",
    "\n",
    "result = whisper.decode(model, mel, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product table is unique product to us.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
